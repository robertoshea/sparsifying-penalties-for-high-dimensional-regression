#Comparing Sparse Penalisation Approaches

#make directories for saving
if(T){
  
  if(!dir.exists('figures')){
    dir.create('figures')
  }
  if(!dir.exists('tables')){
    dir.create('tables')
  }
  if(!dir.exists('all_experiments')){
    dir.create('all_experiments')
  }
}

#load libraries
if(T){
  library(L0Learn)
  library(GEOquery)
  if(packageVersion("GEOquery")=="2.60.0"){
    readr::local_edition(1)
  }
  library(caret)
  library(glmnet)
  library(reshape2)
  library(ggplot2)
  library(huge)
  library(RColorBrewer)
}

#functions
if(T){
  
  qrdc_func <- function(X){
    q <- qr(X)
    X_dc <- X[,q$pivot[seq(q$rank)]]
    return(X_dc)
  }
  
  cos_sim <- function(a, b){
    sum(a*b)/sqrt(sum(a^2)*sum(b^2))
  }
  
  pve_func <- function(y, y_hat){
    1-(mean((y-y_hat)^2)/mean((y-mean(y))^2))
  }
  
  rr_func <- function(opt_y_hat, y_hat){
    mean((opt_y_hat-y_hat)^2)/mean(opt_y_hat^2)
  }
  
  rte_func <- function(y, y_hat, noise_var){
    mean((y-y_hat)^2)/noise_var
  }
  
}

#import data
if(T){
  
  if(T){
    
    #Create Temporary File
    tmp <- tempfile()
    
    #BreastCa
    if(T){
      message('downloading GSE73002...')
      GSE73002 <- getGEO("GSE73002")
      X_GSE73002 <- t(exprs(GSE73002[[1]]))
    }
    
    #LungCa
    if(T){
      message('downloading GSE137140...')
      GSE137140 <- getGEO("GSE137140")
      X_GSE137140 <- t(exprs(GSE137140[[1]]))
    }
    
    #HeadNeckCa
    if(T){
      
      message('downloading GSE103322...')
      GSE103322_url <- 'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE103322&format=file&file=GSE103322%5FHNSCC%5Fall%5Fdata%2Etxt%2Egz'
      download.file(GSE103322_url, tmp)
      GSE103322 <- read.delim(gzfile(tmp),
                              row.names = 1,
                              skip=2)
      colnames(GSE103322) <- NULL
      GSE103322 <- GSE103322[-c(2,3),]
      GSE103322[] <- lapply(GSE103322, as.numeric)
      GSE103322 <- as.matrix(GSE103322)
      GSE103322 <- t(GSE103322)
      X_GSE103322 <- GSE103322[,-1]
      colnames(X_GSE103322) <- substr(colnames(X_GSE103322),
                                      start=2, stop=nchar(colnames(X_GSE103322))-1)
      mean_non_zero <- colMeans(X_GSE103322!=0)
      most_complete_cols <- order(mean_non_zero, decreasing=T)[1:1000]
      X_GSE103322 <- X_GSE103322[,most_complete_cols]
      
    }
    
    #OvarianCa
    if(T){
      message('downloading GSE146026...')
      GSE146026_url <- 'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE146026&format=file&file=GSE146026%5FIzar%5FHGSOC%5Fascites%5F10x%5Flog%2Etsv%2Egz'
      download.file(GSE146026_url, tmp)
      GSE146026 <- read.delim(gzfile(tmp),
                              row.names = 1)
      GSE146026 <-  GSE146026[-(1:7),]
      GSE146026[] <- lapply(GSE146026, as.numeric)
      GSE146026 <- as.matrix(GSE146026)
      X_GSE146026 <- t(GSE146026)
      mean_non_zero <- colMeans(X_GSE146026!=0)
      most_complete_cols <- order(mean_non_zero, decreasing=T)[1:1000]
      X_GSE146026 <- X_GSE146026[,most_complete_cols]
    }
    
    #Astrocytoma
    if(T){
      message('downloading GSE89567...')
      GSE89567_url <- 'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE89567&format=file&file=GSE89567%5FIDH%5FA%5Fprocessed%5Fdata%2Etxt%2Egz'
      download.file(GSE89567_url, tmp)
      GSE89567 <- read.delim(gzfile(tmp),
                             row.names = 1)
      GSE89567 <- as.matrix(GSE89567)
      X_GSE89567 <- t(GSE89567)
      mean_non_zero <- colMeans(X_GSE89567!=0)
      most_complete_cols <- order(mean_non_zero, decreasing=T)[1:1000]
      X_GSE89567 <- X_GSE89567[,most_complete_cols]
    }
    
    data_list <- list(GSE73002=X_GSE73002,
                      GSE137140=X_GSE137140,
                      GSE103322=X_GSE103322,
                      GSE146026=X_GSE146026,
                      GSE89567=X_GSE89567
    )
    
    #apply non-paranormal transformation
    data_list <- lapply(data_list, huge.npn)
    
    #perform qr decompostion to retain a full column rank matrix
    data_list <- lapply(data_list, qrdc_func)
    
    #rename variables and observaations
    data_list <- lapply(data_list, function(i){
      colnames(i) <- paste0("v", 1:ncol(i))
      rownames(i) <- paste0("sample_", 1:nrow(i))
      return(i)
    })
    
    save(data_list, file="data_list.RData")
  }
  if(F){
    load("data_list.RData")
  }
  
}

#data description
if(T){
  
  dataset_descriptions <- data.frame(
    GEO_Accession=c("GSE73002", "GSE137140", "GSE103322", "GSE146026", "GSE89567"),
    Observations=sapply(data_list, nrow),
    Variables=sapply(data_list, ncol),
    Type=rep(c("miRNA", "scRNA"), times=c(2,3))
  )
  rownames(dataset_descriptions)<- NULL
  
  write.csv(dataset_descriptions,
            'tables/dataset_descriptions.csv',
            row.names = F)
}

#setup experiment parameters
if(T){
  param_df <- expand.grid(
    dataset = names(data_list),
    n_train = c(25, 75, 150),
    rep = 1:100,
    nv=500,
    n_folds=c(5,10)
  )
}

#run experiments
if(T){
  
  set.seed(0)
  all_experiments_start_time <- Sys.time()
  all_experiments <- list()
  penalty_types <- c("L0", "L0L1", "L0L2", "L1", "L1L2")
  for(params_i in 1:nrow(param_df)){
    
    message(params_i, "...")
    
    #get parameters for this experiment
    params_list <- param_df[params_i,]
    n_train <- params_list$n_train
    dataset <- params_list$dataset
    type <- params_list$type
    nv <- params_list$nv
    n_folds <- params_list$n_folds
    folds <- sample(rep(1:n_folds, length.out=n_train))
    
    #sample training and test data
    X_all <- data_list[[dataset]]
    train_obs <- sample(nrow(X_all), size=n_train)
    test_obs <- setdiff(1:nrow(X_all), train_obs)
    
    sample_vars <- sample(ncol(X_all), size=nv+1)
    X <- X_all[,sample_vars[-1]]
    X_train <- X[train_obs,]
    X_test <- X[test_obs,]
    y <- X_all[,sample_vars[1]]
    y_train <- y[train_obs]
    y_test <- y[test_obs]
    
    #fit model to all observations
    full_model <- lm(y~.,
                     data=data.frame(X,y=y))
    noise_var <-  var(full_model$residuals)
    signal_var <- var(y)-noise_var
    snr <- signal_var/noise_var
    y_test_opt <- full_model$fitted.values[test_obs]
    
    #infer significant coefficients
    true_coefs <- coef(full_model)[-1]
    coef_pvals <- summary(full_model)$coefficients[-1,4]
    coef_pvals_all <- true_coefs
    coef_pvals_fdr <- p.adjust(coef_pvals, method="fdr")
    significant_coefs <- factor(coef_pvals_fdr<0.05, levels = c("FALSE", "TRUE"))
    n_nonzero <- sum(significant_coefs=="TRUE")
    
    #infer penalised models using training data
    experiment_i <- lapply(penalty_types, function(penalty_i){
      
      message(penalty_i)
      
      #Train Model
      if(penalty_i%in%c("L0", "L0L1", "L0L2")){
        
        start_time <- Sys.time()
        full_mod <- L0Learn.fit(x=X_train,
                                y=y_train,
                                penalty=penalty_i,
                                loss="SquaredError",
                                intercept=F)
        hp_grid <- cbind(
          lambda=unlist(full_mod$lambda),
          gamma=rep(full_mod$gamma,
                    times=sapply(full_mod$lambda,
                                 length)))
        
        
        cv_mses <- lapply(1:n_folds, function(fold_i){
          X_tt <- X_train[folds!=fold_i,]
          X_tv <- X_train[folds==fold_i,]
          y_tt <- y_train[folds!=fold_i]
          y_tv <- y_train[folds==fold_i]
          mod_i <- L0Learn.fit(x=X_tt,
                               y=y_tt,
                               penalty=penalty_i,
                               loss="SquaredError",
                               intercept=F,
                               lambdaGrid=full_mod$lambda)
          cv_mse_i <- sapply(1:nrow(hp_grid), function(hp_i){
            yhat_i <- predict(mod_i,
                              X_tv,
                              lambda=hp_grid[hp_i,1],
                              gamma=hp_grid[hp_i,2]
            )
            cv_mse_ij <- mean((yhat_i-y_tv)^2)
            return(cv_mse_ij)
          })
          return(cv_mse_i)
        })
        cv_mses <- Reduce("+", cv_mses)/n_folds
        cv_err <- min(cv_mses)
        selected_params <- hp_grid[which.min(cv_mses),]
        run_time <- difftime(Sys.time(),
                             start_time,
                             units = "secs")
        opt_lambda <- selected_params[1]
        opt_gamma <- selected_params[2]
        
        nonzero_L0 <- opt_lambda>0
        if(penalty_i=="L0"){
          nonzero_L1 <- F
          nonzero_L2 <- F
        }
        if(penalty_i=="L0L1"){
          nonzero_L1 <- opt_gamma > 0
          nonzero_L2 <- F
        }
        if(penalty_i=="L0L2"){
          nonzero_L1 <- F
          nonzero_L2 <- opt_gamma > 0
        }
        
        beta_hat <- coef(full_mod, lambda=opt_lambda, gamma=opt_gamma)
        
        #multiple models may be returned if they have equivalent coefficients
        if(ncol(beta_hat)>1){
          beta_hat <- c(beta_hat[,1])
        }else{
          beta_hat <- c(as.matrix(beta_hat))
        }
      }
      if(penalty_i=="L1"){
        
        start_time <- Sys.time()
        cv_fit <- cv.glmnet(x=X_train,
                            y=y_train,
                            foldid=folds,
                            intercept=F,
                            lambda.min.ratio=1e-4
        )
        run_time <- difftime(Sys.time(),
                             start_time,
                             units = "secs")
        opt_lambda <- cv_fit$lambda.min
        cv_err <- cv_fit$cvm[cv_fit$lambda==opt_lambda]
        beta_hat <- as.matrix(coef(cv_fit, lambda=opt_lambda))[-1]
        
        nonzero_L0 <- F
        nonzero_L1 <- opt_lambda>0
        nonzero_L2 <- F
        
      }
      if(penalty_i=="L1L2"){
        
        start_time <- Sys.time()
        alpha_vec <- seq(from=0,
                         to=1,
                         length.out=10)
        foldid <- sample(rep(1:n_folds, length.out=n_train))
        cv_fits <- lapply(alpha_vec, function(alpha_i){
          cv.glmnet(x=X_train,
                    y=y_train,
                    foldid=foldid,
                    alpha=alpha_i,
                    intercept=F,
                    lambda.min.ratio=1e-4)
        })
        run_time <- difftime(Sys.time(),
                             start_time,
                             units = "secs")
        
        opt_alpha_idx <- which.min(sapply(cv_fits, function(cv_fit_i){
          min(cv_fit_i$cvm)[1]
        }))[1]
        cv_fit <- cv_fits[[opt_alpha_idx]]
        opt_lambda <- cv_fit$lambda.min
        cv_err <- cv_fit$cvm[cv_fit$lambda==opt_lambda]
        beta_hat <- as.matrix(coef(cv_fit))[-1]
        
        nonzero_L0 <- F
        nonzero_L1 <- opt_lambda>0 & (opt_alpha_idx<1)
        nonzero_L2 <- opt_lambda>0 & (opt_alpha_idx>0)
        
      }
      
      #make predictions
      y_test_hat <- X_test %*% as.matrix(beta_hat)
      
      
      #measure test performance
      rr_test <- rr_func(opt_y_hat = y_test_opt,
                         y_hat = y_test_hat
      )
      rte_test <- rte_func(y = y_test,
                           y_hat = y_test_hat,
                           noise_var = noise_var)
      pve_test <- pve_func(y = y_test,
                           y_hat = y_test_hat)
      
      #variable selection performance
      selection_performance <- caret::confusionMatrix(
        data=factor(beta_hat!=0, levels = c("FALSE", "TRUE")),
        reference=significant_coefs,
        positive="TRUE"
      )$byClass[c("Precision", "Recall", "F1")]
      
      beta_similarity <- cos_sim(beta_hat, true_coefs)
      
      output_list <- list(experiment=params_i,
                          penalty=penalty_i,
                          n_train =n_train,
                          dataset=dataset,
                          snr=snr,
                          rr=rr_test,
                          rte=rte_test,
                          pve=pve_test,
                          precision=selection_performance[1],
                          recall=selection_performance[2],
                          f1=selection_performance[3],
                          beta_similarity=beta_similarity,
                          time=run_time,
                          n_nonzero_coefs = n_nonzero,
                          n_nonzero_coefs_hat = sum(beta_hat!=0),
                          nonzero_L0=nonzero_L0,
                          nonzero_L1=nonzero_L1,
                          nonzero_L2=nonzero_L2,
                          cv_err=cv_err
      )
      return(output_list)
    })
    experiment_i <- do.call(rbind.data.frame, experiment_i)
    
    
    #update list
    all_experiments[[params_i]] <- experiment_i
    
  }
  all_experiments_run_time <-  difftime(Sys.time(),
                                        all_experiments_start_time,
                                        units = "hours")
  
  
}

#save experiments
if(T){
  save(all_experiments, file="all_experiments.RData")
}

#reload experiments
if(T){
  load("all_experiments.RData")
}

#reformat experiment data
if(T){
  
  all_experiments_rbound <- do.call(rbind, all_experiments)
  all_experiments_rbound$n_train <- paste0("n=",  all_experiments_rbound$n_train)
  all_experiments_rbound$n_train <- factor(all_experiments_rbound$n_train,
                                           levels=unique(all_experiments_rbound$n_train))
  all_experiments_rbound$dataset <- factor(all_experiments_rbound$dataset)
  all_experiments_melted <- all_experiments_rbound
  
  all_experiments_melted <- melt(all_experiments_melted, id=c("experiment", "penalty", "n_train","dataset"))
  colnames(all_experiments_melted)<- gsub("variable", "metric",
                                          colnames(all_experiments_melted)) 
  
  all_experiments_melted$value[is.na(all_experiments_melted$value)&
                                 all_experiments_melted$metric%in%c(
                                   "precision", "recall", "f1", "beta_similarity"
                                 )
  ] <- 0
  all_experiments_melted$value[is.nan(all_experiments_melted$value)&
                                 all_experiments_melted$metric%in%c(
                                   "precision", "recall", "f1", "beta_similarity"
                                 )
  ] <- 0
  all_experiments_melted$metric <- gsub("beta_similarity", "coefficient_similarity", all_experiments_melted$metric )
  
  
}

#compare performance of preselection and preselection validation
if(T){
  
  #identify optimal models using other datasets
  preselected_methods <- all_experiments_melted[,c("dataset", "n_train", "metric")]
  preselected_methods <- preselected_methods[!duplicated(preselected_methods),]
  comparison_metrics <- c("pve", "f1", "coefficient_similarity")
  preselected_methods <- preselected_methods[preselected_methods$metric %in% comparison_metrics,]
  preselected_methods$method <- sapply(1:nrow(preselected_methods), function(i){
    
    params_i <- preselected_methods[i,]
    
    relevant_experiments <- all_experiments_melted[
      all_experiments_melted$dataset!=params_i$dataset&
        all_experiments_melted$n_train==params_i$n_train&
        all_experiments_melted$metric==params_i$metric,]
    relevant_experiments$value_rank <- NA
    #rank selection method performance for each experiment
    for(start_i in seq(1, nrow(relevant_experiments), 5)){
      idx_i <- start_i:(start_i + 4)
      relevant_experiments$value_rank[idx_i] <- rank(relevant_experiments$value[idx_i])
    }
    #select the best overall model for the experimental conditions
    method_scores <- aggregate(relevant_experiments$value_rank,
                               by=relevant_experiments[,"penalty", drop=F],
                               FUN=mean)
    selected_method <- method_scores$penalty[which.max(method_scores$x)]
    return(selected_method)
  })
  
  #Extract performance of preselected and internally selected models
  preselected_vs_internal_performance <- lapply(all_experiments, function(experiment_i){
    
    colnames(experiment_i) <- gsub("beta_similarity",
                                   "coefficient_similarity",
                                   colnames(experiment_i))
    colnames(experiment_i) <- gsub("F1",
                                   "f1",
                                   colnames(experiment_i))
    
    comparison_i <- preselected_methods[
      preselected_methods$dataset==experiment_i$dataset[1]&
        preselected_methods$n_train==paste0("n=", experiment_i$n_train[1]),
    ]
    colnames(comparison_i)[4] <- "preselection_method"
    
    comparison_i$preselection_performance <- as.numeric(experiment_i[cbind(
      match(comparison_i$preselection_method, experiment_i$penalty),
      match(comparison_i$metric, colnames(experiment_i), )
    )])
    comparison_i$internal_method <- experiment_i$penalty[which.min(experiment_i$cv_err)]
    
    comparison_i$internal_performance <- unlist(experiment_i[
      which.min(experiment_i$cv_err),
      match(comparison_i$metric, colnames(experiment_i))
    ])
    comparison_i$preselection_performance[is.nan(comparison_i$preselection_performance)]<- 0
    comparison_i$preselection_performance[is.na(comparison_i$preselection_performance)]<- 0
    comparison_i$internal_performance[is.nan(comparison_i$internal_performance)]<- 0
    comparison_i$internal_performance[is.na(comparison_i$internal_performance)]<- 0
    
    return(comparison_i)
    
  })
  preselected_vs_internal_performance <- do.call(rbind,
                                                 preselected_vs_internal_performance)
  preselected_vs_internal_performance$best_selector <- 
    ifelse(preselected_vs_internal_performance$preselection_performance>
             preselected_vs_internal_performance$internal_performance,
           "preselection",
           "internal_validation")
  preselected_vs_internal_performance$best_selector[
    preselected_vs_internal_performance$preselection_performance==
      preselected_vs_internal_performance$internal_performance
  ] <- "equivalent"
  
  #Compare preselected results to models selected by internal validation
  comparison_tests <- lapply(comparison_metrics, function(metric_i){
    
    preselection_performance_i  <-
      preselected_vs_internal_performance$preselection_performance[
        preselected_vs_internal_performance$metric==metric_i
      ]
    internal_performance_i <-
      preselected_vs_internal_performance$internal_performance[
        preselected_vs_internal_performance$metric==metric_i
      ]
    
    test_i <- t.test(preselection_performance_i,
                          internal_performance_i,
                          paired = T,
                          alternative = "two.sided",
                          conf.int = T
    )
    output_i <- data.frame(
      metric=metric_i,
      t=format(round(test_i$statistic, 2), nsmall=2),
      df=test_i$parameter,
      Diff= paste0(format(round(test_i$estimate, 3), nsmall=3)," [",
                   format(round(test_i$conf.int[1], 3), nsmall=3), ", ",
                   format(round(test_i$conf.int[2], 3), nsmall=3), "]"),
      p=test_i$p.value
    )
    return(output_i)
  })
  comparison_tests <- do.call(rbind, comparison_tests)
  write.table(comparison_tests,
              file='tables/preselection_comparison_tests.tsv',
              row.names = F,
              sep='\t')
  
  comparison_df <- preselected_vs_internal_performance[c("metric",
                                                         "preselection_performance",
                                                         "internal_performance")]
  comparison_df$difference <- comparison_df$preselection_performance - comparison_df$internal_performance
  comparison_df$metric2 <- c("Proportion Variance Explained","F1", "Coefficient Similarity")[factor(comparison_df$metric,
                                                               levels=c( "pve", "f1", "coefficient_similarity")
                                                               )]

  
}

#plotting
if(T){
  
  reg_metrics <- c("rr", "pve")
  reg_df <- all_experiments_melted[all_experiments_melted$metric%in%reg_metrics,]
  reg_df$metric2 <- factor(c("Proportion Variance Explained", "Relative Risk")[factor(reg_df$metric)])
  reg_plot <- ggplot(
    data=reg_df,
    aes(x=n_train,y=value, colour=penalty))+
    geom_boxplot(coef=NULL)+
    facet_grid(metric2~.,  scales="free_y")+
    ylab(NULL)+
    xlab(NULL)+
    scale_color_brewer(palette = "Dark2")
  reg_plot$labels$colour <- "Penalty"
  
  ggsave(plot=reg_plot,
         filename='figures/reg_plot.tiff',
         dpi=300,
         units='in',
         width = 5,
         height = 5)
  
  vs_metrics <- c("precision","recall","f1","coefficient_similarity")
  vs_df <- all_experiments_melted[all_experiments_melted$metric%in%vs_metrics,]
  vs_df$metric2 <- factor(c("Coefficient Similarity", "F1 Score", "Precision", "Recall")[factor(vs_df$metric)],
                          levels = c("Coefficient Similarity", "Precision", "Recall","F1 Score")
  )
  vs_plot <- ggplot(
    data=vs_df,
    aes(x=n_train,y=value, colour=penalty))+
    geom_boxplot(coef=NULL)+
    facet_grid(metric2~., scales="free_y")+
    ylab(NULL)+
    xlab(NULL)+
    scale_color_brewer(palette = "Dark2")
  vs_plot$labels$colour <- "Penalty"
  
  ggsave(plot=vs_plot,
         filename='figures/vs_plot.tiff',
         dpi=300,
         units='in',
         width = 5,
         height = 6)
  
  pve_snr_df <- all_experiments_rbound[,c("penalty", "n_train", "pve", "snr")]
  pve_noise_plot <- ggplot(
    data=pve_snr_df,
    aes(x=pve,y=snr, colour=penalty))+
    geom_point()+
    facet_grid(penalty~n_train)+
    ylab("Signal:Noise Ratio")+
    xlab("Proportion of Variance Explained")+
    geom_vline(xintercept=0, linetype="dashed")+
    theme(legend.position = "None")+
    scale_color_brewer(palette = "Dark2")
  
  ggsave(plot=pve_noise_plot,
         filename='figures/pve_noise_plot.tiff',
         dpi=300,
         units='in',
         width = 5,
         height = 6)
    
  comparison_ecdf_plot <- 
    ggplot(comparison_df, aes(x=difference)) +
    stat_ecdf(geom="step",color="red", size=0.8)+
    geom_vline(xintercept=0, linetype=2)+
    facet_wrap(metric2~., scales="free")+
    xlab("Preselection Performance Minus Internal Validation Performance")+
    ylab("Quantile")
  ggsave(plot=comparison_ecdf_plot,
         filename='figures/comparison_ecdf_plot.tiff',
         dpi=300,
         units='in',
         width = 8,
         height = 4)
  
  
  
}

#result tables
if(T){
  
  all_performance_metrics <- c(reg_metrics, vs_metrics, "n_nonzero_coefs_hat")
  all_performance_metrics_verbose <-c("Relative risk", "Relative test error", "Proportion of variance explained",
                                      "Precision", "Recall", "F1 score", "Coefficient Similarity", "Nonzero Coefficients")
  all_performance_summary <- all_experiments_melted[all_experiments_melted$metric%in%all_performance_metrics, ]
  all_performance_summary$metric <- all_performance_metrics_verbose[
    factor(all_performance_summary$metric, levels=all_performance_metrics)]
  
  median_performance <- aggregate(all_performance_summary$value,
                                  by=all_performance_summary[,c("penalty","n_train", "metric")],
                                  FUN=function(i){
                                    formatC(median(i), digits=2, format="f")
                                  })
  colnames(median_performance)[4] <- "Median"
  median_performance$IQR <- aggregate(all_performance_summary$value,
                                      by=all_performance_summary[,c("penalty","n_train", "metric")],
                                      FUN=function(i){
                                        iqr=formatC(quantile(i, probs = c(0.25, 0.75)), digits=2, format="f")
                                        paste0("[", iqr[1], ", ", iqr[2], "]")
                                      })$x
  
  total_median_performance <- aggregate(all_experiments_melted$value,
                                        by=all_experiments_melted[,c("penalty", "metric")],
                                        FUN=median)
  
  all_performance_summary <- median_performance
  colnames(all_performance_summary)[1:3]<- c("Penalty", "N", "Metric")
  all_performance_summary$N <- gsub("n=", "",all_performance_summary$N)
  
  
  predictive_performance_summary <- all_performance_summary[
    all_performance_summary$Metric %in%
    c("Relative risk", "Relative test error", "Proportion of variance explained"),
  ]
  vs_performance_summary <- all_performance_summary[
    all_performance_summary$Metric %in%
    c("Precision", "Recall", "F1 score", "Coefficient Similarity", "Nonzero Coefficients"),
  ]
  
  write.table(all_performance_summary,
              file='tables/all_performance_summary.tsv',
              row.names = F,
              sep='\t')
  write.table(predictive_performance_summary,
              file='tables/predictive_performance_summary.tsv',
              row.names = F,
              sep='\t')
  write.table(vs_performance_summary,
              file='tables/vs_performance_summary.tsv',
              row.names = F,
              sep='\t')
  
  pve_snr_test <- cor.test(all_experiments_rbound$snr, all_experiments_rbound$pve)
  pve_snr_test_text <- paste0("($\rho$: ", round(pve_snr_test$estimate, 2),
                              ", 95% CI: [",  round(pve_snr_test$conf.int[1], 2),
                              ", ", round(pve_snr_test$conf.int[2], 2),
                              "], _P_", format.pval(pve_snr_test$p.value, digits=2)
  )
  
  
}

#Reporting functions
if(T){
  all_datasets <- unique(all_experiments_melted$dataset)
  all_n_train <- unique(all_experiments_melted$n_train)
  all_penalties <- unique(all_experiments_melted$penalty)
  sf <- function(metric=NULL,
                 penalty=all_penalties,
                 n_train=all_n_train,
                 dataset=all_datasets,
                 digits=2){
    
    vec <- all_experiments_melted[
      all_experiments_melted$metric==metric&
        all_experiments_melted$penalty%in%penalty&
        all_experiments_melted$n_train%in%n_train&
        all_experiments_melted$dataset%in%dataset,
    ]$value
    vec <- quantile(vec, probs=c(0.5, 0.25, 0.75))
    vec <- formatC(vec, digits=digits, format="f")
    vec <- paste0("(Median: ", vec[1], ", IQR: [", vec[2], ", ", vec[3], "])")
    return(vec)
  }
  
  minf <- function(metric=NULL,
                   penalty=all_penalties,
                   n_train=all_n_train,
                   dataset=all_datasets,
                   digits=2){
    
    vec <- all_experiments_melted[
      all_experiments_melted$metric==metric&
        all_experiments_melted$penalty%in%penalty&
        all_experiments_melted$n_train%in%n_train&
        all_experiments_melted$dataset%in%dataset,
    ]$value
    vec <- quantile(vec, probs=c(0, 0.05))
    vec <- formatC(vec, digits=digits, format="f")
    vec <- paste0("(Min: ", vec[1], ", 5th Quantile: ", vec[2], ")")
    return(vec)
  }
  
  maxf <- function(metric=NULL,
                   penalty=all_penalties,
                   n_train=all_n_train,
                   dataset=all_datasets,
                   digits=2){
    
    vec <- all_experiments_melted[
      all_experiments_melted$metric==metric&
        all_experiments_melted$penalty%in%penalty&
        all_experiments_melted$n_train%in%n_train&
        all_experiments_melted$dataset%in%dataset,
    ]$value
    vec <- quantile(vec, probs=c(1, 0.95))
    vec <- formatC(vec, digits=digits, format="f")
    vec <- paste0("(Max: ", vec[1], ", 95th Quantile: ", vec[2], ")")
    return(vec)
  }
  
  
}

#save data
if(F){
  save.image(file = "comparing_sparse_penalties.RData")
}

